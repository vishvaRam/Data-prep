{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c53382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset,DatasetDict\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a227591",
   "metadata": {},
   "outputs": [],
   "source": [
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Vishva007/RBI-Circular-QA-Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174beb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_eval_df = df.head(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da9022",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split_df = initial_eval_df.sample(n=100, random_state=42).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8cff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of the initial evaluation split: {eval_split_df.shape}\")\n",
    "print(eval_split_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RephrasedContent(BaseModel):\n",
    "    rephrased_question: str = Field(description=\"The rephrased version of the original financial question.\")\n",
    "    rephrased_answer: str = Field(description=\"The rephrased version of the original financial answer, maintaining factual accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", google_api_key=os.environ.get(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffddfdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rephrase_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant specialized in rephrasing RBI financial questions and answers. \n",
    "                Rephrase the provided question to make it slightly different but retain its original meaning. \n",
    "                Also, rephrase the provided answer to make it slightly different while retaining its original meaning and factual accuracy.\"\"\"),\n",
    "    (\"user\", \"Question: {question}\\nAnswer: {answer}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3e7e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_rephrase_chain = combined_rephrase_prompt | model.with_structured_output(RephrasedContent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ff1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split_df['rephrased_question'] = \"\"\n",
    "eval_split_df['rephrased_answer'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea94cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the sampled DataFrame and rephrase\n",
    "for index, row in eval_split_df.iterrows():\n",
    "    original_question = row['question']\n",
    "    original_answer = row['answer']\n",
    "\n",
    "    try:\n",
    "        # Invoke the combined chain\n",
    "        rephrased_output = combined_rephrase_chain.invoke({\n",
    "            \"question\": original_question,\n",
    "            \"answer\": original_answer\n",
    "        })\n",
    "        \n",
    "        eval_split_df.loc[index, 'rephrased_question'] = rephrased_output.rephrased_question\n",
    "        eval_split_df.loc[index, 'rephrased_answer'] = rephrased_output.rephrased_answer\n",
    "        \n",
    "        print(f\"Processed sample {index+1}/{len(eval_split_df)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sample {index}: {e}\")\n",
    "        eval_split_df.loc[index, 'rephrased_question'] = f\"Error: {e}\"\n",
    "        eval_split_df.loc[index, 'rephrased_answer'] = f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11653e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nRephrasing complete. Displaying the first few rows of the rephrased evaluation split with all columns:\")\n",
    "# Print the entire eval_split_df to see all columns\n",
    "print(eval_split_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5809a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Prepare the new 'eval' split and update the DatasetDict ---\n",
    "print(\"\\nStep 4: Preparing new 'eval' split and updating DatasetDict...\")\n",
    "\n",
    "# Convert the rephrased DataFrame to a Hugging Face Dataset\n",
    "new_eval_hf_dataset = Dataset.from_pandas(eval_split_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32c11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_hf_dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65fdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns_to_add = ['rephrased_question', 'rephrased_answer']\n",
    "train_features = original_train_hf_dataset.features.keys()\n",
    "columns_to_actually_add = [col for col in new_columns_to_add if col not in train_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if columns_to_actually_add:\n",
    "    print(f\"Adding missing columns {columns_to_actually_add} to the 'train' split for feature alignment...\")\n",
    "    original_train_hf_dataset = original_train_hf_dataset.map(\n",
    "        lambda examples: {col: [\"\"] * len(examples[\"question\"]) for col in columns_to_actually_add}, # Initialize with empty strings\n",
    "        batched=True # Process in batches for efficiency\n",
    "    )\n",
    "else:\n",
    "    print(\"Train split already has 'rephrased_question' and 'rephrased_answer' columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3c1bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features of 'train' split after alignment:\")\n",
    "print(original_train_hf_dataset.features)\n",
    "print(\"\\nFeatures of 'eval' split:\")\n",
    "print(new_eval_hf_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a137f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset_dict = DatasetDict({\n",
    "    'train': original_train_hf_dataset,\n",
    "    'eval': new_eval_hf_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6ecb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Updated DatasetDict splits: {updated_dataset_dict.keys()}\")\n",
    "print(f\"Number of samples in 'train' split: {len(updated_dataset_dict['train'])}\")\n",
    "print(f\"Number of samples in 'eval' split: {len(updated_dataset_dict['eval'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4353d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_dataset_dict['eval'][0]\n",
    "\n",
    "# {'document': 'RBI_2023-2024_65CO.DGBA.GBD.No.S646_42-01-029_2023-2024_2023-10-03',\n",
    "#  'filename': 'RBI_2023-2024_65CO.DGBA.GBD.No.S646_42-01-029_2023-2024_2023-10-03_text_part1.txt',\n",
    "#  'model_name': 'models/gemini-2.0-flash',\n",
    "#  'regulation_area': 'Government Transactions',\n",
    "#  'applicable_to': 'All Agency Banks',\n",
    "#  'issued_on': '2023-10-03',\n",
    "#  'key_topics': ['Government transactions through e-Kuber',\n",
    "#   'Processing of transactions on March 31, 2024',\n",
    "#   'Accounting of transactions for FY 2023-24'],\n",
    "#  'chunks_text': \"![](_page_0_Picture_0.jpeg)\\n\\n![](_page_0_Picture_1.jpeg)\\n\\n![](_page_0_Picture_3.jpeg)\\n\\nRBI/2023-24/65 CO.DGBA.GBD.No.S646/42-01-029/2023-2024 October 03, 2023\\n\\nAll Agency Banks\\n\\nMadam/Dear Sir,\\n\\n## **Status of March 31, 2024 for Government transactions through integration with e-Kuber**\\n\\nThe 'e-Kuber' which is the Core Banking Solution platform of RBI for Government and other payments does not process any Government transactions on Global holidays (which are 26th January, 15th August, 2nd October, all 2nd and 4th Saturdays of a month and on all Sundays). It is observed that March 31, 2024 falls on a Sunday. The office of Controller General of Accounts, Government of India has advised that in order to account for all the Government transactions relating to receipts and payments in the financial year 2023-24 itself, it has been decided that March 31, 2024 (Sunday) be marked as a working day for the Government transactions so that all the Government transactions through integration with e-Kuber are processed on March 31, 2024 and accounted for in the financial year 2023-24 itself for arriving the cash balance of Government of India as on March 31, 2024.\\n\\n2. Also, the luggage files from banks for transferring the data related to Government transactions to RBI would also be accepted by e-Kuber system on March 31, 2024 for accounting of the same in the account for the financial year 2023-24.\\n\\nYours faithfully\\n\\n(Indranil Chakraborty) Chief General Manager\\n\\n![](_page_0_Picture_15.jpeg)\",\n",
    "#  'is_table': False,\n",
    "#  'question': 'Why was March 31, 2024 designated as a working day for government transactions in India?',\n",
    "#  'answer': \"The Controller General of Accounts, Government of India, advised that March 31, 2024, be marked as a working day to ensure all government receipts and payments are accounted for in the financial year 2023-24. This allows for the accurate calculation of the Government of India's cash balance as of March 31, 2024.\",\n",
    "#  'evaluation_criteria': 'Answer should accurately state the reason for designating March 31, 2024, as a working day, including the purpose of accounting for government transactions and calculating the cash balance.',\n",
    "#  'category': 'fact-based',\n",
    "#  'estimated_difficulty': 4,\n",
    "#  'rephrased_question': 'What prompted the designation of March 31, 2024, as a working day for government financial activities in India?',\n",
    "#  'rephrased_answer': \"To guarantee that all governmental income and outgoings were incorporated into the 2023-24 fiscal year, the Controller General of Accounts of the Indian government suggested designating March 31, 2024, as a working day. This ensures an exact computation of the Indian government's cash reserves as they stood on March 31, 2024.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c11ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 5: Push the updated DatasetDict to Hugging Face Hub ---\n",
    "print(\"\\nStep 5: Pushing the updated dataset to Hugging Face Hub...\")\n",
    "repo_id = \"Vishva007/RBI-Circular-QA-Dataset\" # Your dataset repo ID on Hugging Face Hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8284f134",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Push the updated DatasetDict to the Hugging Face Hub\n",
    "    # set push_latest=True to overwrite if the split already exists in the repo\n",
    "    updated_dataset_dict.push_to_hub(repo_id, private=False) # Set private=True if you want it private\n",
    "\n",
    "    print(f\"\\nDataset '{repo_id}' successfully updated on Hugging Face Hub with 'eval' split!\")\n",
    "    print(\"You can view it here: https://huggingface.co/datasets/\" + repo_id)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError pushing to Hugging Face Hub: {e}\")\n",
    "    print(\"Please ensure you are logged in to Hugging Face Hub (`huggingface-cli login` or `notebook_login()`)\")\n",
    "    print(f\"Also, verify you have write permissions for '{repo_id}' or choose a new `repo_id` under your namespace.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
